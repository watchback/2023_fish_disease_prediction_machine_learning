{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54228a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "plt.rcParams['font.family'] = 'AppleGothic' # For Mac\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC # 의사결정나무 분류 알고리즘\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score , recall_score , confusion_matrix\n",
    "from sklearn.metrics import accuracy_score # 정확도 함수\n",
    "import pickle\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_validate\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4934b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/csb/Desktop/Project/넙치 머신러닝/0_양식어류 질병예측_221207/2안.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d22746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = data[['Temperature', 'ConditionFactor', 'T P', 'Wgp2', 'Wgi3', 'Whkp2', 'Whki2-1', 'Wlr1', 'Wlr2-1', 'Wsi2','CLASS']]\n",
    "data = data[['사료급이량','백신접종여부','DO','임상증상유무','기생충유무','BUN','CREA','GLU','Whkr7','Whkp2','Whki2-1','Wlr2-1','Wsi2','결과']]\n",
    "#data = data[['사료급이량', '백신접종여부', 'DO', '임상증상유무', '기생충유무', 'BUN', 'CREA', 'GLU', 'Head kidney', 'Liver ', 'Spleen','결과']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ec1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    \"Normal\":0,\n",
    "    \"Warning\": 1,\n",
    "    \"Infection\": 2,\n",
    "    # 다른 클래스 레이블에 대한 매핑도 추가 가능\n",
    "}\n",
    "data['CLASS'] = data['CLASS'].map(class_mapping)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d8292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# features/target분리\n",
    "\n",
    "feature_columns = list(data.columns.difference(['결과']))\n",
    "X = data[feature_columns]\n",
    "y = data['결과']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247df5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# 표준 스케일러(평균 0, 분산 1)\n",
    "scaler = StandardScaler()\n",
    "# 설명변수 데이터 스케일링\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20ef569",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.1, random_state = 10) # 학습데이터와 평가데이터의 비율을 8:2 로 분할| \n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'max_features': [3,4,5,6,7,9,11]}]\n",
    "\n",
    "forest = RandomForestClassifier(random_state=1234)\n",
    "\n",
    "grid_search = GridSearchCV(forest, param_grid, cv=10,\n",
    "                           scoring='accuracy',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb6850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('최적 하이퍼 파라미터: ', grid_search.best_params_)\n",
    "print('최고 예측 정확도: {:.4f}'.format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67923924",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/csb/Desktop/Project/넙치 머신러닝/0_양식어류 질병예측_221207/23년.csv\")\n",
    "data = data[['수온', 'Condition factor', 'ALP', 'T P', 'Wgp2', 'Wgi3', 'Whkp2', 'Whki2-1', 'Wlr1', 'Wlr2-1', 'Wsi2','결과']]\n",
    "feature_columns = list(data.columns.difference(['결과']))\n",
    "test_x = data[feature_columns]\n",
    "test_y = data['결과']\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f9148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = scaler.fit_transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9261fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score # 정확도 함수\n",
    "\n",
    "forest = RandomForestClassifier(random_state = 1234)\n",
    "# max_features = n_features,min_samples_leaf = n_leaf,min_samples_split = n_split,max_depth = n_depth\n",
    "forest.fit(X, y)\n",
    "\n",
    "#y_pred = forest.predict(test_x)\n",
    "\n",
    "#print('테스트세트 정확도: {:.3f}'.format(forest.score(test_x, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca313706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "ftr_importances_values = forest.feature_importances_\n",
    "ftr_importances = pd.Series(ftr_importances_values, index = X.columns)\n",
    "ftr_top20 = ftr_importances.sort_values(ascending=False)[:40]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Feature Importances')\n",
    "sns.barplot(x=ftr_top20, y=ftr_top20.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a740de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimi_visualization(algorithm_name, x_values, train_score, test_score, xlabel, filename):\n",
    "    # 하이퍼파라미터 조정에 따른 학습 데이터셋 기반 모델 성능 추이 시각화\n",
    "    plt.plot(x_values, train_score, linestyle = '-', label = 'train score')\n",
    "    # 하이퍼파라미터 조정에 따른 테스트 데이터셋 기반 모델 성능 추이 시각화\n",
    "    plt.plot(x_values, test_score, linestyle = '--', label = 'test score')\n",
    "    plt.ylabel('Accuracy(%)') # y축 라벨\n",
    "    plt.xlabel(xlabel) # x축 라벨\n",
    "    plt.legend() # 범례표시\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5515a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_estimator(algorithm, algorithm_name, x_train, y_train, x_test, y_test, feature_min, feature_max):\n",
    "    train_score = []; test_score =[]\n",
    "    para_n_tree = [n_tree for n_tree in range(feature_min, feature_max)]\n",
    "\n",
    "    for v_n_features in para_n_tree:\n",
    "        model = algorithm(max_features = v_n_features, random_state=1234)\n",
    "        model.fit(x_train, y_train)\n",
    "        train_score.append(model.score(x_train, y_train))\n",
    "        test_score.append(model.score(x_test, y_test))\n",
    "\n",
    "    # 트리 개수에 따른 모델 성능 저장\n",
    "    df_score_n = pd.DataFrame({'n_features': para_n_tree, 'TrainScore': train_score, 'TestScore': test_score})\n",
    "    # 트리 개수에 따른 모델 성능 추이 시각화 함수 호출\n",
    "    optimi_visualization(algorithm_name, para_n_tree, train_score, test_score, \"The number of feature\", \"max_feature\")\n",
    "    print(round(df_score_n, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c7e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimi_estimator(algorithm, algorithm_name, x_train, y_train, x_test, y_test, n_estimator_min, n_estimator_max,n_features):\n",
    "    train_score = []; test_score =[]\n",
    "    para_n_tree = [n_tree*5 for n_tree in range(n_estimator_min, n_estimator_max)]\n",
    "\n",
    "    for v_n_estimators in para_n_tree:\n",
    "        model = algorithm(n_estimators = v_n_estimators,\n",
    "                          max_features = n_features,\n",
    "                          random_state=1234)\n",
    "        model.fit(x_train, y_train)\n",
    "        train_score.append(model.score(x_train, y_train))\n",
    "        test_score.append(model.score(x_test, y_test))\n",
    "\n",
    "    # 트리 개수에 따른 모델 성능 저장\n",
    "    df_score_n = pd.DataFrame({'n_estimators': para_n_tree, 'TrainScore': train_score, 'TestScore': test_score})\n",
    "    # 트리 개수에 따른 모델 성능 추이 시각화 함수 호출\n",
    "    optimi_visualization(algorithm_name, para_n_tree, train_score, test_score, \"The number of estimator\", \"n_estimator\")\n",
    "    print(round(df_score_n, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfbb8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimi_maxdepth (algorithm, algorithm_name, x_train, y_train, x_test, y_test, depth_min, depth_max,n_features, n_estimator):\n",
    "    train_score = []; test_score = []\n",
    "    para_depth = [depth for depth in range(depth_min, depth_max)]\n",
    "\n",
    "    for v_max_depth in para_depth:\n",
    "        # 의사결정나무 모델의 경우 트리 개수를 따로 설정하지 않기 때문에 RFC, GBC와 분리하여 모델링\n",
    "        if algorithm == DTC:\n",
    "            model = algorithm(max_depth = v_max_depth,\n",
    "                              random_state=1234)\n",
    "        else:\n",
    "            model = algorithm(max_depth = v_max_depth,\n",
    "                              n_estimators = n_estimator,\n",
    "                              max_features = n_features,\n",
    "                              random_state=1234)\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        train_score.append(model.score(x_train, y_train))\n",
    "        test_score.append(model.score(x_test, y_test))\n",
    "\n",
    "    # 최대 깊이에 따른 모델 성능 저장\n",
    "    df_score_n = pd.DataFrame({'depth': para_depth, 'TrainScore': train_score, 'TestScore': test_score})\n",
    "    # 최대 깊이에 따른 모델 성능 추이 시각화 함수 호출\n",
    "    optimi_visualization(algorithm_name, para_depth, train_score, test_score, \"The number of depth\", \"n_depth\")\n",
    "    print(round(df_score_n, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2339a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimi_minsplit (algorithm, algorithm_name, x_train, y_train, x_test, y_test, n_split_min, n_split_max,n_features, n_estimator, n_depth):\n",
    "    train_score = []; test_score = []\n",
    "    para_split = [n_split*2 for n_split in range(n_split_min, n_split_max)]\n",
    "    for v_min_samples_split in para_split:\n",
    "        # 의사결정나무 모델의 경우 트리 개수를 따로 설정하지 않기 때문에 RFC, GBC와 분리하여 모델링\n",
    "        if algorithm == DTC:\n",
    "            model = algorithm(min_samples_split = v_min_samples_split,\n",
    "                              max_depth = n_depth,\n",
    "                              random_state = 1234)\n",
    "        else:\n",
    "            model = algorithm(min_samples_split = v_min_samples_split,\n",
    "                              n_estimators = n_estimator,\n",
    "                              max_features = n_features,\n",
    "                              max_depth = n_depth,\n",
    "                              random_state = 1234)\n",
    "        model.fit(x_train, y_train)\n",
    "        train_score.append(model.score(x_train, y_train))\n",
    "        test_score.append(model.score(x_test, y_test))\n",
    "\n",
    "    # 분리 노드의 최소 자료 수에 따른 모델 성능 저장\n",
    "    df_score_n = pd.DataFrame({'min_samples_split': para_split, 'TrainScore': train_score, 'TestScore': test_score})\n",
    "    # 분리 노드의 최소 자료 수에 따른 모델 성능 추이 시각화 함수 호출\n",
    "    optimi_visualization(algorithm_name, para_split, train_score, test_score, \"The minimum number of samples required to split an internal node\", \"min_samples_split\")\n",
    "    print(round(df_score_n, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ee573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimi_minleaf(algorithm, algorithm_name, x_train, y_train, x_test, y_test, n_leaf_min, n_leaf_max,n_features, n_estimator, n_depth, n_split):\n",
    "    train_score = []; test_score = []\n",
    "    para_leaf = [n_leaf*2 for n_leaf in range(n_leaf_min, n_leaf_max)]\n",
    "\n",
    "    for v_min_samples_leaf in para_leaf:\n",
    "        # 의사결정나무 모델의 경우 트리 개수를 따로 설정하지 않기 때문에 RFC, GBC와 분리하여 모델링\n",
    "        if algorithm == DTC:\n",
    "            model = algorithm(min_samples_leaf = v_min_samples_leaf,\n",
    "                                        max_depth = n_depth,\n",
    "                                        min_samples_split = n_split,\n",
    "                                        random_state=1234)\n",
    "        else:\n",
    "            model = algorithm(min_samples_leaf = v_min_samples_leaf,\n",
    "                                n_estimators = n_estimator,\n",
    "                              max_features = n_features,\n",
    "                                max_depth = n_depth,\n",
    "                                min_samples_split = n_split,\n",
    "                                random_state=1234)\n",
    "        model.fit(x_train, y_train)\n",
    "        train_score.append(model.score(x_train, y_train))\n",
    "        test_score.append(model.score(x_test, y_test))\n",
    "\n",
    "    # 잎사귀 노드의 최소 자료 수에 따른 모델 성능 저장\n",
    "    df_score_n = pd.DataFrame({'min_samples_leaf': para_leaf, 'TrainScore': train_score, 'TestScore': test_score})\n",
    "    # 잎사귀 노드의 최소 자료 수에 따른 모델 성능 추이 시각화 함수 호출\n",
    "    optimi_visualization(algorithm_name, para_leaf, train_score, test_score, \"The minimum number of samples required to be at a leaf node\", \"min_samples_leaf\")\n",
    "    print(round(df_score_n, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebdefef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_min = 1\n",
    "feature_max = 12\n",
    "feature_estimator(RandomForestClassifier, 'RF', \n",
    "                 train_x, train_y, test_x, test_y,\n",
    "                 feature_min, feature_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdfb800",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27173f41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_estimator_min = 1\n",
    "n_estimator_max = 51\n",
    "optimi_estimator(RandomForestClassifier, 'RF', \n",
    "                 train_x, train_y, test_x, test_y, \n",
    "                 n_estimator_min, n_estimator_max,n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f75d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimator = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87481eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "depth_min = 1\n",
    "depth_max = 21\n",
    "optimi_maxdepth(RandomForestClassifier, 'RF',\n",
    "                train_x, train_y, test_x, test_y,\n",
    "                depth_min, depth_max,n_features, n_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedd14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_depth=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c9fd9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_split_min = 1\n",
    "n_split_max = 101\n",
    "# 데이터프레임 행 최대 100개까지 반드시 출력\n",
    "pd.set_option('display.max_row', 100)\n",
    "optimi_minsplit (RandomForestClassifier, 'RF',\n",
    "                train_x, train_y, test_x, test_y,\n",
    "                 n_split_min, n_split_max,n_features, n_estimator, n_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f803c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_split = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd221be",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_leaf_min = 1\n",
    "n_leaf_max = 51\n",
    "optimi_minleaf(RandomForestClassifier, 'RF',\n",
    "                train_x, train_y, test_x, test_y, \n",
    "               n_leaf_min, n_leaf_max,n_features, n_estimator, n_depth, n_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_leaf=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_final(algorithm, algorithm_name, feature_name, x_train, y_train, x_test, y_test, n_features, n_estimator, n_depth, n_split, n_leaf):\n",
    "    # 의사결정나무 모델의 경우 트리 개수를 따로 설정하지 않기 때문에 RFC, GBC와 분리하여 모델링\n",
    "    if algorithm == DTC:\n",
    "        model = algorithm(random_state=1234, \n",
    "                          max_features = n_features,\n",
    "                          min_samples_leaf = n_leaf,\n",
    "                          min_samples_split = n_split, \n",
    "                          max_depth = n_depth)\n",
    "    else:\n",
    "        model = algorithm(random_state = 1234,\n",
    "                          max_features = n_features,\n",
    "                          n_estimators = n_estimator, \n",
    "                          min_samples_leaf = n_leaf,\n",
    "                          min_samples_split = n_split, \n",
    "                          max_depth = n_depth)\n",
    "    # 모델 학습\n",
    "    model.fit(x_train, y_train)\n",
    "    # 모델 저장\n",
    "    model_path = '/Users/csb/Desktop/'\n",
    "    model_filename = 'wine_classification_' + algorithm_name + '.pkl'\n",
    "    with open(model_path + model_filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"최종 모델 저장 완료! 파일 경로: {model_path + model_filename}\\n\")\n",
    "    \n",
    "    # 최종 모델의 성능 평가\n",
    "    train_acc = model.score(x_train, y_train)\n",
    "    test_acc = model.score(x_test, y_test)\n",
    "    y_pred = model.predict(x_test)\n",
    "    precision = precision_score(y_test, y_pred,average= \"macro\")\n",
    "    recall = recall_score(y_test, y_pred,average= \"macro\")\n",
    "    F1score = f1_score(y_test, y_pred,average= \"macro\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\") # 정확도\n",
    "    print(f\"Precision: {precision:.3f}\") # 정밀도\n",
    "    print(f\"Recall: {recall:.3f}\") # 재현율\n",
    "    print(f\"F1-score: {F1score:.3f}\") # F1 스코어\n",
    "    \n",
    "    # 혼동행렬 시각화\n",
    "    plt.figure(figsize =(30, 30))\n",
    "    plot_confusion_matrix(model, \n",
    "                         test_x, test_y,\n",
    "                         include_values = True,\n",
    "                         display_labels = ['정상', '주의','질병'], # 목표변수 이름\n",
    "                         cmap = 'Pastel1') # 컬러맵\n",
    "    plt.savefig('/Users/csb/Desktop/' + algorithm_name + '_confusion_matrix.png') # 혼동행렬 자료 저장\n",
    "    plt.show()\n",
    "    \n",
    "    # 변수 중요도 산출\n",
    "    dt_importance = pd.DataFrame()\n",
    "    dt_importance['Feature'] = feature_name # 설명변수 이름\n",
    "    dt_importance['Importance'] = model.feature_importances_ # 설명변수 중요도 산출\n",
    "\n",
    "    # 변수 중요도 내림차순 정렬\n",
    "    dt_importance.sort_values(\"Importance\", ascending = False, inplace = True)\n",
    "    print(dt_importance.round(3))\n",
    "    # 변수 중요도 오름차순 정렬\n",
    "    dt_importance.sort_values(\"Importance\", ascending = True, inplace = True)\n",
    "    # 변수 중요도 시각화\n",
    "    coordinates = range(len(dt_importance)) # 설명변수 개수만큼 bar 시각화\n",
    "    plt.barh(y = coordinates, width = dt_importance[\"Importance\"])\n",
    "    plt.yticks(coordinates, dt_importance[\"Feature\"]) # y축 눈금별 설명변수 이름 기입\n",
    "    plt.xlabel(\"Feature Importance\") # x축 이름\n",
    "    plt.ylabel(\"Features\") # y축 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693a565",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_final(RandomForestClassifier, 'RF', X.columns,\n",
    "            train_x, train_y, test_x, test_y,\n",
    "            n_features,n_estimator, n_depth, n_split, n_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb30d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state = 1234)\n",
    "loocv = LeaveOneOut()\n",
    "score = cross_val_score(forest, X, y, cv = loocv,scoring=\"accuracy\")\n",
    "\n",
    "print(\"\\n## 교차 검증 횟수 : \", len(score))\n",
    "print(\"교차 검증 평균 : {:.3f}\".format(score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae888c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 프레임 불러오기 (rawData에 해당하는 데이터를 사용)\n",
    "# 예를 들어, 'rawData.csv' 파일을 불러오는 경우:\n",
    "# rawData = pd.read_csv('rawData.csv')\n",
    "\n",
    "# 결과를 저장할 데이터 프레임 초기화\n",
    "n_rows, n_cols = data.shape\n",
    "predictResults = pd.DataFrame(columns=['Normal', 'warning',\"infection\", 'Actual_CLASS', 'Predicted_CLASS'])\n",
    "\n",
    "# 반복해서 각 행을 처리\n",
    "for i in range(n_rows):\n",
    "    # 현재 행을 테스트 데이터로 사용\n",
    "    test_data = data.iloc[i, :-1].values.reshape(1, -1)\n",
    "    actual_class = data.iloc[i, -1]\n",
    "\n",
    "    # 현재 행을 제외한 나머지 행을 훈련 데이터로 사용\n",
    "    train_data = data.drop(i, axis=0)\n",
    "\n",
    "    # 훈련 데이터와 레이블을 나누기\n",
    "    X = train_data.iloc[:, :-1]  # 모든 열 중 마지막 열을 제외한 열\n",
    "    y = train_data.iloc[:, -1]  # 마지막 열 (CLASS 열)\n",
    "\n",
    "    # 랜덤 포레스트 모델 초기화\n",
    "    n_trees = 100\n",
    "    rf = RandomForestClassifier(n_estimators=n_trees, random_state=1234)\n",
    "\n",
    "    # 모델 학습\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    # 예측\n",
    "    predicted_prob = rf.predict_proba(test_data)[0]\n",
    "\n",
    "    predicted_class = rf.predict(test_data)[0]\n",
    "\n",
    "    # 결과 저장\n",
    "    predictResults.loc[i] = [predicted_prob[0], predicted_prob[1], predicted_prob[2], actual_class, predicted_class]\n",
    "\n",
    "# 결과 출력\n",
    "print(predictResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666bbb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = predictResults['Actual_CLASS']\n",
    "predicted = predictResults['Predicted_CLASS']\n",
    "\n",
    "confusion_mat = confusion_matrix(actual, predicted)\n",
    "\n",
    "# 결과 출력\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852400ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal 클래스에 대한 계산\n",
    "normalTP = confusion_mat[0, 0]\n",
    "normalFP = np.sum(confusion_mat[1:3, 0])\n",
    "normalTN = np.sum(confusion_mat[1:3, 1:3])\n",
    "normalFN = np.sum(confusion_mat[0, 1:3])\n",
    "\n",
    "# Warning 클래스에 대한 계산\n",
    "warningTP = confusion_mat[1, 1]\n",
    "warningFP = np.sum(confusion_mat[0::2, 1])\n",
    "warningTN = np.sum(confusion_mat[0::2, 0::2])\n",
    "warningFN = np.sum(confusion_mat[1, 0::2])\n",
    "\n",
    "# Infection 클래스에 대한 계산\n",
    "infectionTP = confusion_mat[2, 2]\n",
    "infectionFP = np.sum(confusion_mat[0:2, 2])\n",
    "infectionTN = np.sum(confusion_mat[0:2, 0:2])\n",
    "infectionFN = np.sum(confusion_mat[2, 0:2])\n",
    "\n",
    "# 전체 계산\n",
    "TP = normalTP + warningTP + infectionTP\n",
    "FP = normalFP + warningFP + infectionFP\n",
    "TN = normalTN + warningTN + infectionTN\n",
    "FN = normalFN + warningFN + infectionFN\n",
    "\n",
    "# 결과 출력\n",
    "print(\"TP:\", TP)\n",
    "print(\"FP:\", FP)\n",
    "print(\"TN:\", TN)\n",
    "print(\"FN:\", FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e7ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_micro = (TP + TN) / (TP + FP + TN + FN)\n",
    "sensitivity_micro = TP / (TP + FN)\n",
    "specificity_micro = TN / (TN + FP)\n",
    "ppv_micro = TP / (TP + FP)\n",
    "npv_micro = TN / (TN + FN)\n",
    "f1_micro = (2 * TP) / ((2 * TP) + FP + FN)\n",
    "\n",
    "mcc_micro = ((TP * TN) - (FP * FN)) / (np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)))\n",
    "\n",
    "# 결과 출력\n",
    "print(\"Accuracy (Micro):\", accuracy_micro)\n",
    "print(\"Sensitivity (Micro):\", sensitivity_micro)\n",
    "print(\"Specificity (Micro):\", specificity_micro)\n",
    "print(\"Positive Predictive Value (Micro):\", ppv_micro)\n",
    "print(\"Negative Predictive Value (Micro):\", npv_micro)\n",
    "print(\"F1 Score (Micro):\", f1_micro)\n",
    "print(\"Matthews Correlation Coefficient (Micro):\", mcc_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29fd9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''forest = RandomForestClassifier(random_state = 1234,\n",
    "                          max_features = n_features,\n",
    "                          n_estimators = n_estimator, \n",
    "                          min_samples_leaf = n_leaf,\n",
    "                          min_samples_split = n_split,\n",
    "                          max_depth = n_depth)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c3cb70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy = score['test_accuracy']\n",
    "precision = score['test_precision_macro']\n",
    "recall = score['test_recall_macro']\n",
    "f1 = score['test_f1_macro']\n",
    "print(\"교차 검증 점수 평균 : {:.3f}\".format(accuracy.mean()))\n",
    "print(\"Precision : {:.3f}\".format(precision.mean()))\n",
    "print(\"Recall : {:.3f}\".format(recall.mean()))\n",
    "print(\"F1 : {:.3f}\".format(f1.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "score = cross_val_score(xgb, X, y, cv = loocv,scoring='accuracy')\n",
    "print(\"\\n## 교차 검증 횟수 : \", len(score))\n",
    "print(\"교차 검증 점수 평균 : {:.3f}\".format(score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bb474e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
